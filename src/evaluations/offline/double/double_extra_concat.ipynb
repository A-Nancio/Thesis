{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Dropout, GRU\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from machine_learning.models import DoubleExtraConcatProduction\n",
    "from data_processing.batch_generator import load_test_set\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DoubleExtraConcatProduction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryAccuracy(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.AUC()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.49214274]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transaction = np.load(f'{path}data/test/all_transactions.npy')[0]\n",
    "test_labels = np.load(f'{path}data/test/all_labels.npy').astype(float)\n",
    "test_set = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.load(f'{path}data/test/all_transactions.npy'), \n",
    "     test_labels)\n",
    ").batch(1)\n",
    "# initialize weights\n",
    "model(np.expand_dims(sample_transaction, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.822193</td>\n",
       "      <td>0.964010</td>\n",
       "      <td>62.0</td>\n",
       "      <td>982376.0</td>\n",
       "      <td>31842.0</td>\n",
       "      <td>4836.0</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.595133</td>\n",
       "      <td>0.003369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029721</td>\n",
       "      <td>0.995143</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1014077.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>4809.0</td>\n",
       "      <td>0.386957</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>0.755983</td>\n",
       "      <td>0.034711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.995287</td>\n",
       "      <td>332.0</td>\n",
       "      <td>1013981.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>4566.0</td>\n",
       "      <td>0.583480</td>\n",
       "      <td>0.067783</td>\n",
       "      <td>0.815763</td>\n",
       "      <td>0.121456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>0.995362</td>\n",
       "      <td>551.0</td>\n",
       "      <td>1013838.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>4347.0</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.112495</td>\n",
       "      <td>0.832571</td>\n",
       "      <td>0.189055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.020596</td>\n",
       "      <td>0.995411</td>\n",
       "      <td>665.0</td>\n",
       "      <td>1013774.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>4233.0</td>\n",
       "      <td>0.599639</td>\n",
       "      <td>0.135770</td>\n",
       "      <td>0.840546</td>\n",
       "      <td>0.221408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.995415</td>\n",
       "      <td>715.0</td>\n",
       "      <td>1013728.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>4183.0</td>\n",
       "      <td>0.593361</td>\n",
       "      <td>0.145978</td>\n",
       "      <td>0.846273</td>\n",
       "      <td>0.234311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>0.995442</td>\n",
       "      <td>742.0</td>\n",
       "      <td>1013729.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>4156.0</td>\n",
       "      <td>0.602762</td>\n",
       "      <td>0.151490</td>\n",
       "      <td>0.847661</td>\n",
       "      <td>0.242128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.995482</td>\n",
       "      <td>776.0</td>\n",
       "      <td>1013736.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>4122.0</td>\n",
       "      <td>0.616852</td>\n",
       "      <td>0.158432</td>\n",
       "      <td>0.848132</td>\n",
       "      <td>0.252112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.995478</td>\n",
       "      <td>789.0</td>\n",
       "      <td>1013719.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>4109.0</td>\n",
       "      <td>0.612578</td>\n",
       "      <td>0.161086</td>\n",
       "      <td>0.847086</td>\n",
       "      <td>0.255092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.019928</td>\n",
       "      <td>0.995475</td>\n",
       "      <td>798.0</td>\n",
       "      <td>1013706.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>0.609160</td>\n",
       "      <td>0.162924</td>\n",
       "      <td>0.848257</td>\n",
       "      <td>0.257088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.019899</td>\n",
       "      <td>0.995518</td>\n",
       "      <td>828.0</td>\n",
       "      <td>1013720.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>4070.0</td>\n",
       "      <td>0.624434</td>\n",
       "      <td>0.169049</td>\n",
       "      <td>0.848080</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.019936</td>\n",
       "      <td>0.995506</td>\n",
       "      <td>828.0</td>\n",
       "      <td>1013708.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>4070.0</td>\n",
       "      <td>0.618834</td>\n",
       "      <td>0.169049</td>\n",
       "      <td>0.845962</td>\n",
       "      <td>0.265555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.995468</td>\n",
       "      <td>826.0</td>\n",
       "      <td>1013671.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>4072.0</td>\n",
       "      <td>0.601602</td>\n",
       "      <td>0.168640</td>\n",
       "      <td>0.845379</td>\n",
       "      <td>0.263435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.995454</td>\n",
       "      <td>831.0</td>\n",
       "      <td>1013652.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>4067.0</td>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.169661</td>\n",
       "      <td>0.845511</td>\n",
       "      <td>0.264019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>0.995474</td>\n",
       "      <td>838.0</td>\n",
       "      <td>1013665.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>4060.0</td>\n",
       "      <td>0.602444</td>\n",
       "      <td>0.171090</td>\n",
       "      <td>0.843777</td>\n",
       "      <td>0.266497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.020133</td>\n",
       "      <td>0.995470</td>\n",
       "      <td>820.0</td>\n",
       "      <td>1013679.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>0.603385</td>\n",
       "      <td>0.167415</td>\n",
       "      <td>0.846633</td>\n",
       "      <td>0.262106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.020064</td>\n",
       "      <td>0.995474</td>\n",
       "      <td>868.0</td>\n",
       "      <td>1013635.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>0.598208</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.845155</td>\n",
       "      <td>0.273429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.995486</td>\n",
       "      <td>867.0</td>\n",
       "      <td>1013649.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>4031.0</td>\n",
       "      <td>0.603760</td>\n",
       "      <td>0.177011</td>\n",
       "      <td>0.846941</td>\n",
       "      <td>0.273761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.019994</td>\n",
       "      <td>0.995474</td>\n",
       "      <td>852.0</td>\n",
       "      <td>1013651.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>4046.0</td>\n",
       "      <td>0.600423</td>\n",
       "      <td>0.173949</td>\n",
       "      <td>0.845694</td>\n",
       "      <td>0.269748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.019872</td>\n",
       "      <td>0.995497</td>\n",
       "      <td>866.0</td>\n",
       "      <td>1013661.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>0.608573</td>\n",
       "      <td>0.176807</td>\n",
       "      <td>0.846382</td>\n",
       "      <td>0.274007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss  binary_accuracy     TP         TN       FP      FN   \n",
       "0       0  0.822193         0.964010   62.0   982376.0  31842.0  4836.0  \\\n",
       "1       1  0.029721         0.995143   89.0  1014077.0    141.0  4809.0   \n",
       "2       2  0.023730         0.995287  332.0  1013981.0    237.0  4566.0   \n",
       "3       3  0.021634         0.995362  551.0  1013838.0    380.0  4347.0   \n",
       "4       4  0.020596         0.995411  665.0  1013774.0    444.0  4233.0   \n",
       "5       5  0.020300         0.995415  715.0  1013728.0    490.0  4183.0   \n",
       "6       6  0.020122         0.995442  742.0  1013729.0    489.0  4156.0   \n",
       "7       7  0.019966         0.995482  776.0  1013736.0    482.0  4122.0   \n",
       "8       8  0.020005         0.995478  789.0  1013719.0    499.0  4109.0   \n",
       "9       9  0.019928         0.995475  798.0  1013706.0    512.0  4100.0   \n",
       "10     10  0.019899         0.995518  828.0  1013720.0    498.0  4070.0   \n",
       "11     11  0.019936         0.995506  828.0  1013708.0    510.0  4070.0   \n",
       "12     12  0.020055         0.995468  826.0  1013671.0    547.0  4072.0   \n",
       "13     13  0.020107         0.995454  831.0  1013652.0    566.0  4067.0   \n",
       "14     14  0.020110         0.995474  838.0  1013665.0    553.0  4060.0   \n",
       "15     15  0.020133         0.995470  820.0  1013679.0    539.0  4078.0   \n",
       "16     16  0.020064         0.995474  868.0  1013635.0    583.0  4030.0   \n",
       "17     17  0.019985         0.995486  867.0  1013649.0    569.0  4031.0   \n",
       "18     18  0.019994         0.995474  852.0  1013651.0    567.0  4046.0   \n",
       "19     19  0.019872         0.995497  866.0  1013661.0    557.0  4032.0   \n",
       "\n",
       "    precision    recall       auc  f1_score  \n",
       "0    0.001943  0.012658  0.595133  0.003369  \n",
       "1    0.386957  0.018171  0.755983  0.034711  \n",
       "2    0.583480  0.067783  0.815763  0.121456  \n",
       "3    0.591837  0.112495  0.832571  0.189055  \n",
       "4    0.599639  0.135770  0.840546  0.221408  \n",
       "5    0.593361  0.145978  0.846273  0.234311  \n",
       "6    0.602762  0.151490  0.847661  0.242128  \n",
       "7    0.616852  0.158432  0.848132  0.252112  \n",
       "8    0.612578  0.161086  0.847086  0.255092  \n",
       "9    0.609160  0.162924  0.848257  0.257088  \n",
       "10   0.624434  0.169049  0.848080  0.266067  \n",
       "11   0.618834  0.169049  0.845962  0.265555  \n",
       "12   0.601602  0.168640  0.845379  0.263435  \n",
       "13   0.594846  0.169661  0.845511  0.264019  \n",
       "14   0.602444  0.171090  0.843777  0.266497  \n",
       "15   0.603385  0.167415  0.846633  0.262106  \n",
       "16   0.598208  0.177215  0.845155  0.273429  \n",
       "17   0.603760  0.177011  0.846941  0.273761  \n",
       "18   0.600423  0.173949  0.845694  0.269748  \n",
       "19   0.608573  0.176807  0.846382  0.274007  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f'{path}training_runs/train_{model.name}.csv')\n",
    "train_df['f1_score'] = 2 * (train_df['precision'] * train_df['recall']) / (train_df['precision'] + train_df['recall'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.059823</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61348.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498802</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.032590</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61348.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842786</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.031240</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61348.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836806</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.993570</td>\n",
       "      <td>44.0</td>\n",
       "      <td>61299.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.833499</td>\n",
       "      <td>0.181443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.033560</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61299.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.834242</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61299.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.035058</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61299.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.827254</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.036848</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61103.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.821428</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61201.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.824648</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61235.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.302469</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.830630</td>\n",
       "      <td>0.176895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.036044</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61201.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.829147</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.991659</td>\n",
       "      <td>75.0</td>\n",
       "      <td>61150.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.191327</td>\n",
       "      <td>0.821062</td>\n",
       "      <td>0.225564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.036248</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61201.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.829270</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.037659</td>\n",
       "      <td>0.990476</td>\n",
       "      <td>98.0</td>\n",
       "      <td>61054.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.824927</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.037433</td>\n",
       "      <td>0.991999</td>\n",
       "      <td>49.0</td>\n",
       "      <td>61197.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.773530</td>\n",
       "      <td>0.165541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.048228</td>\n",
       "      <td>0.986476</td>\n",
       "      <td>98.0</td>\n",
       "      <td>60807.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.153365</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.800723</td>\n",
       "      <td>0.190107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.036785</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>98.0</td>\n",
       "      <td>61152.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.826914</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.037391</td>\n",
       "      <td>0.992436</td>\n",
       "      <td>72.0</td>\n",
       "      <td>61201.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.825925</td>\n",
       "      <td>0.235679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.037901</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61099.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0.188925</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>0.825499</td>\n",
       "      <td>0.165951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.041912</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>98.0</td>\n",
       "      <td>60956.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.817728</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch      loss  binary_accuracy    TP       TN     FP     FN  precision   \n",
       "0       0  0.059823         0.993651   0.0  61348.0    0.0  392.0   0.000000  \\\n",
       "1       1  0.032590         0.993651   0.0  61348.0    0.0  392.0   0.000000   \n",
       "2       2  0.031240         0.993651   0.0  61348.0    0.0  392.0   0.000000   \n",
       "3       3  0.032012         0.993570  44.0  61299.0   49.0  348.0   0.473118   \n",
       "4       4  0.033560         0.993651  49.0  61299.0   49.0  343.0   0.500000   \n",
       "5       5  0.034069         0.993651  49.0  61299.0   49.0  343.0   0.500000   \n",
       "6       6  0.035058         0.993651  49.0  61299.0   49.0  343.0   0.500000   \n",
       "7       7  0.036848         0.990476  49.0  61103.0  245.0  343.0   0.166667   \n",
       "8       8  0.036170         0.992063  49.0  61201.0  147.0  343.0   0.250000   \n",
       "9       9  0.036031         0.992614  49.0  61235.0  113.0  343.0   0.302469   \n",
       "10     10  0.036044         0.992063  49.0  61201.0  147.0  343.0   0.250000   \n",
       "11     11  0.037475         0.991659  75.0  61150.0  198.0  317.0   0.274725   \n",
       "12     12  0.036248         0.992063  49.0  61201.0  147.0  343.0   0.250000   \n",
       "13     13  0.037659         0.990476  98.0  61054.0  294.0  294.0   0.250000   \n",
       "14     14  0.037433         0.991999  49.0  61197.0  151.0  343.0   0.245000   \n",
       "15     15  0.048228         0.986476  98.0  60807.0  541.0  294.0   0.153365   \n",
       "16     16  0.036785         0.992063  98.0  61152.0  196.0  294.0   0.333333   \n",
       "17     17  0.037391         0.992436  72.0  61201.0  147.0  320.0   0.328767   \n",
       "18     18  0.037901         0.990557  58.0  61099.0  249.0  334.0   0.188925   \n",
       "19     19  0.041912         0.988889  98.0  60956.0  392.0  294.0   0.200000   \n",
       "\n",
       "      recall       auc  f1_score  \n",
       "0   0.000000  0.498802       NaN  \n",
       "1   0.000000  0.842786       NaN  \n",
       "2   0.000000  0.836806       NaN  \n",
       "3   0.112245  0.833499  0.181443  \n",
       "4   0.125000  0.834242  0.200000  \n",
       "5   0.125000  0.787240  0.200000  \n",
       "6   0.125000  0.827254  0.200000  \n",
       "7   0.125000  0.821428  0.142857  \n",
       "8   0.125000  0.824648  0.166667  \n",
       "9   0.125000  0.830630  0.176895  \n",
       "10  0.125000  0.829147  0.166667  \n",
       "11  0.191327  0.821062  0.225564  \n",
       "12  0.125000  0.829270  0.166667  \n",
       "13  0.250000  0.824927  0.250000  \n",
       "14  0.125000  0.773530  0.165541  \n",
       "15  0.250000  0.800723  0.190107  \n",
       "16  0.250000  0.826914  0.285714  \n",
       "17  0.183673  0.825925  0.235679  \n",
       "18  0.147959  0.825499  0.165951  \n",
       "19  0.250000  0.817728  0.222222  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv(f'{path}training_runs/validation_{model.name}.csv')\n",
    "val_df['f1_score'] = 2 * (val_df['precision'] * val_df['recall']) / (val_df['precision'] + val_df['recall'])\n",
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epoch with the best performance on validation is **16**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
